{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jun 18 21:09:59 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 531.14                 Driver Version: 531.14       CUDA Version: 12.1     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                      TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA RTX A6000              WDDM | 00000000:47:00.0 Off |                  Off |\n",
      "| 30%   36C    P8               25W / 300W|    537MiB / 49140MiB |      5%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      1696    C+G   ...Programs\\Microsoft VS Code\\Code.exe    N/A      |\n",
      "|    0   N/A  N/A      4224    C+G   C:\\Program Files\\RustDesk\\rustdesk.exe    N/A      |\n",
      "|    0   N/A  N/A      4632    C+G   ...ekyb3d8bbwe\\PhoneExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A      5076    C+G   C:\\Windows\\explorer.exe                   N/A      |\n",
      "|    0   N/A  N/A      7748    C+G   ...crosoft\\Edge\\Application\\msedge.exe    N/A      |\n",
      "|    0   N/A  N/A      9912    C+G   ...siveControlPanel\\SystemSettings.exe    N/A      |\n",
      "|    0   N/A  N/A     10800    C+G   ...HP\\Sure Click\\servers\\BrConsole.exe    N/A      |\n",
      "|    0   N/A  N/A     11576    C+G   ...nt.CBS_cw5n1h2txyewy\\SearchHost.exe    N/A      |\n",
      "|    0   N/A  N/A     11600    C+G   ...2txyewy\\StartMenuExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     11848    C+G   ...41.0_x64__v10z8vjag6ke6\\HP.myHP.exe    N/A      |\n",
      "|    0   N/A  N/A     16512    C+G   ...CBS_cw5n1h2txyewy\\TextInputHost.exe    N/A      |\n",
      "|    0   N/A  N/A     17660    C+G   ...5n1h2txyewy\\ShellExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     17776    C+G   ...cal\\Microsoft\\OneDrive\\OneDrive.exe    N/A      |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bitsandbytes\n",
    "!pip install accelerate\n",
    "!pip install peft\n",
    "!pip install wandb\n",
    "!pip install datasets\n",
    "!pip install trl\n",
    "!pip install einops\n",
    "!pip install tokenizers\n",
    "!pip install tiktoken\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install bitsandbytes-cuda121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pprint import pprint\n",
    "\n",
    "import bitsandbytes as bnb\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import notebook_login\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftConfig,\n",
    "    PeftModel,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    ")\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3eee78bbbe2d4307a0a5e0230e02e776",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "notebook_login()\n",
    "#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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\BMSCE CSE\\Desktop\\Instruct\\myenv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c02a6b2c19634955b6e8ce1f081bc7c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MODEL_NAME = \"tiiuae/falcon-7b-instruct\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    #trust_remote_code=True,\n",
    "    quantization_config=bnb_config,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 9437184 || all params: 3618182016 || trainable%: 0.260826679207064\n"
     ]
    }
   ],
   "source": [
    "config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"query_key_value\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<human>: What does a cluster definition file contain??\n",
      "<assistant>:\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "<human>: What does a cluster definition file contain??\n",
    "<assistant>:\n",
    "\"\"\".strip()\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerationConfig {\n",
       "  \"bos_token_id\": 11,\n",
       "  \"eos_token_id\": 11,\n",
       "  \"max_new_tokens\": 200,\n",
       "  \"pad_token_id\": 11,\n",
       "  \"temperature\": 0.7,\n",
       "  \"top_p\": 0.7\n",
       "}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation_config = model.generation_config\n",
    "generation_config.max_new_tokens = 200\n",
    "generation_config.temperature = 0.7\n",
    "generation_config.top_p = 0.7\n",
    "generation_config.num_return_sequences = 1\n",
    "generation_config.pad_token_id = tokenizer.eos_token_id\n",
    "generation_config.eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "\n",
    "generation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\BMSCE CSE\\Desktop\\Instruct\\myenv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\BMSCE CSE\\Desktop\\Instruct\\myenv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\BMSCE CSE\\Desktop\\Instruct\\myenv\\Lib\\site-packages\\transformers\\models\\falcon\\modeling_falcon.py:446: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = F.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<human>: What does a cluster definition file contain??\n",
      "<assistant>: A cluster definition file contains information about the cluster, such as the cluster name, the number of nodes in the cluster, and the configuration settings for the cluster.\n",
      "User \n",
      "CPU times: total: 3.09 s\n",
      "Wall time: 3.16 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "device = \"cuda:0\"\n",
    "\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "encoding = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "with torch.inference_mode():\n",
    "    outputs = model.generate(\n",
    "        input_ids=encoding.input_ids,\n",
    "        attention_mask=encoding.attention_mask,\n",
    "        generation_config=generation_config,\n",
    "    )\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 677\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "import random\n",
    "\n",
    "csv_file_path = './train_data.csv'\n",
    "\n",
    "# Load the CSV file into a pandas DataFrame\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Create a new DataFrame with the extracted text\n",
    "new_df = pd.DataFrame({'text': df['text']})\n",
    "\n",
    "# Create a Dataset object\n",
    "dataset = Dataset.from_pandas(new_df)\n",
    "\n",
    "# Shuffle the dataset\n",
    "random.shuffle(dataset['text'])\n",
    "\n",
    "# Print the shuffled dataset\n",
    "print(dataset)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'### Human 2: What lines should be added at the end of the /etc/chrony.conf file for NTP broadcasting? ### Assistant 2: Add lines for NTP server identification and NTP broadcasting to the management network and BMC network.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset\n",
    "dataset[\"text\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 677\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "dataset_dict = DatasetDict({'train': dataset})\n",
    "\n",
    "dataset=dataset_dict\n",
    "# Print the dataset_dict\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(data_point):\n",
    "    return f\"\"\"\n",
    "<human>: {data_point[\"question\"]}\n",
    "<assistant>: {data_point[\"answer\"]}\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "def generate_and_tokenize_prompt(data_point):\n",
    "    full_prompt = generate_prompt(data_point)\n",
    "    tokenized_full_prompt = tokenizer(full_prompt, padding=True, truncation=True)\n",
    "    return tokenized_full_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = dataset['text'].shuffle().map(generate_and_tokenize_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = \"Instruct_70_30_split\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "per_device_train_batch_size = 4\n",
    "gradient_accumulation_steps = 4\n",
    "optim = \"paged_adamw_32bit\"\n",
    "save_steps = 10\n",
    "logging_steps = 10\n",
    "learning_rate = 2e-4\n",
    "max_grad_norm = 0.3\n",
    "max_steps = 200\n",
    "warmup_ratio = 0.03\n",
    "lr_scheduler_type = \"constant\"\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    save_steps=save_steps,\n",
    "    logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    fp16=True,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    max_steps=max_steps,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    gradient_checkpointing=True,\n",
    "    \n",
    ")\n",
    "# training_arguments = TrainingArguments(\n",
    "#     output_dir=\"./Instruct_70_30_split\",\n",
    "#     per_device_train_batch_size=1,\n",
    "#     gradient_accumulation_steps=4,\n",
    "#     optim='paged_adamw_32bit',\n",
    "#     save_steps=250,\n",
    "#     fp16=True,\n",
    "#     logging_steps=10,\n",
    "#     learning_rate=2e-4,\n",
    "#     max_grad_norm=0.3,\n",
    "#     max_steps=30000,\n",
    "#     warmup_ratio=0.03,\n",
    "#     lr_scheduler_type=\"constant\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d12ac1e7cdff413ea2aa7c8f5846527c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/677 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "max_seq_length = 1024\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset['train'],\n",
    "    peft_config=config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module name: base_model.model.transformer.h.0.input_layernorm, module type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "Module name: base_model.model.transformer.h.1.input_layernorm, module type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "Module name: base_model.model.transformer.h.2.input_layernorm, module type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "Module name: base_model.model.transformer.h.3.input_layernorm, module type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "Module name: base_model.model.transformer.h.4.input_layernorm, module type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "Module name: base_model.model.transformer.h.5.input_layernorm, module type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "Module name: base_model.model.transformer.h.6.input_layernorm, module type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "Module name: base_model.model.transformer.h.7.input_layernorm, module type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "Module name: base_model.model.transformer.h.8.input_layernorm, module type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "Module name: base_model.model.transformer.h.9.input_layernorm, module type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "Module name: base_model.model.transformer.h.10.input_layernorm, module type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "Module name: base_model.model.transformer.h.11.input_layernorm, module type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "Module name: base_model.model.transformer.h.12.input_layernorm, module type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "Module name: base_model.model.transformer.h.13.input_layernorm, module type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "Module name: base_model.model.transformer.h.14.input_layernorm, module type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "Module name: base_model.model.transformer.h.15.input_layernorm, module type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "Module name: base_model.model.transformer.h.16.input_layernorm, module type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "Module name: base_model.model.transformer.h.17.input_layernorm, module type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "Module name: base_model.model.transformer.h.18.input_layernorm, module type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "Module name: base_model.model.transformer.h.19.input_layernorm, module type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "Module name: base_model.model.transformer.h.20.input_layernorm, module type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "Module name: base_model.model.transformer.h.21.input_layernorm, module type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "Module name: base_model.model.transformer.h.22.input_layernorm, module type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "Module name: base_model.model.transformer.h.23.input_layernorm, module type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "Module name: base_model.model.transformer.h.24.input_layernorm, module type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "Module name: base_model.model.transformer.h.25.input_layernorm, module type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "Module name: base_model.model.transformer.h.26.input_layernorm, module type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "Module name: base_model.model.transformer.h.27.input_layernorm, module type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "Module name: base_model.model.transformer.h.28.input_layernorm, module type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "Module name: base_model.model.transformer.h.29.input_layernorm, module type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "Module name: base_model.model.transformer.h.30.input_layernorm, module type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "Module name: base_model.model.transformer.h.31.input_layernorm, module type: <class 'torch.nn.modules.normalization.LayerNorm'>\n"
     ]
    }
   ],
   "source": [
    "for name, module in trainer.model.named_modules():\n",
    "    if \"norm\" in name:\n",
    "        module = module.to(torch.float32)\n",
    "\n",
    "for name, module in trainer.model.named_modules():\n",
    "    if \"norm\" in name:\n",
    "        print(f\"Module name: {name}, module type: {type(module)}\")\n",
    "        module = module.to(torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maditi-cs21\u001b[0m (\u001b[33mbmsce\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f36d5bf3c1e4c4f862ca2ec8813d3c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011111111111111112, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\BMSCE CSE\\Desktop\\Instruct\\wandb\\run-20240618_211801-khp2fs5t</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bmsce/huggingface/runs/khp2fs5t' target=\"_blank\">serene-hill-27</a></strong> to <a href='https://wandb.ai/bmsce/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bmsce/huggingface' target=\"_blank\">https://wandb.ai/bmsce/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bmsce/huggingface/runs/khp2fs5t' target=\"_blank\">https://wandb.ai/bmsce/huggingface/runs/khp2fs5t</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3aaed2b730a847cfa46748c0037a50bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\BMSCE CSE\\Desktop\\Instruct\\myenv\\Lib\\site-packages\\torch\\utils\\checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.3043, 'grad_norm': 2.4998583793640137, 'learning_rate': 0.0002, 'epoch': 0.24}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\BMSCE CSE\\Desktop\\Instruct\\myenv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\BMSCE CSE\\Desktop\\Instruct\\myenv\\Lib\\site-packages\\torch\\utils\\checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6759, 'grad_norm': 1.8085155487060547, 'learning_rate': 0.0002, 'epoch': 0.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\BMSCE CSE\\Desktop\\Instruct\\myenv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\BMSCE CSE\\Desktop\\Instruct\\myenv\\Lib\\site-packages\\torch\\utils\\checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4101, 'grad_norm': 1.7279399633407593, 'learning_rate': 0.0002, 'epoch': 0.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\BMSCE CSE\\Desktop\\Instruct\\myenv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\BMSCE CSE\\Desktop\\Instruct\\myenv\\Lib\\site-packages\\torch\\utils\\checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2403, 'grad_norm': 2.0576272010803223, 'learning_rate': 0.0002, 'epoch': 0.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\BMSCE CSE\\Desktop\\Instruct\\myenv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\BMSCE CSE\\Desktop\\Instruct\\myenv\\Lib\\site-packages\\torch\\utils\\checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.107, 'grad_norm': 1.508984088897705, 'learning_rate': 0.0002, 'epoch': 1.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\BMSCE CSE\\Desktop\\Instruct\\myenv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\BMSCE CSE\\Desktop\\Instruct\\myenv\\Lib\\site-packages\\torch\\utils\\checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9425, 'grad_norm': 1.6227680444717407, 'learning_rate': 0.0002, 'epoch': 1.41}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\BMSCE CSE\\Desktop\\Instruct\\myenv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\BMSCE CSE\\Desktop\\Instruct\\myenv\\Lib\\site-packages\\torch\\utils\\checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9615, 'grad_norm': 1.1901476383209229, 'learning_rate': 0.0002, 'epoch': 1.65}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\BMSCE CSE\\Desktop\\Instruct\\myenv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\BMSCE CSE\\Desktop\\Instruct\\myenv\\Lib\\site-packages\\torch\\utils\\checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9072, 'grad_norm': 1.6269434690475464, 'learning_rate': 0.0002, 'epoch': 1.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\BMSCE CSE\\Desktop\\Instruct\\myenv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\BMSCE CSE\\Desktop\\Instruct\\myenv\\Lib\\site-packages\\torch\\utils\\checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8204, 'grad_norm': 1.2926549911499023, 'learning_rate': 0.0002, 'epoch': 2.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\BMSCE CSE\\Desktop\\Instruct\\myenv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\BMSCE CSE\\Desktop\\Instruct\\myenv\\Lib\\site-packages\\torch\\utils\\checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.76, 'grad_norm': 1.393326759338379, 'learning_rate': 0.0002, 'epoch': 2.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\BMSCE CSE\\Desktop\\Instruct\\myenv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\BMSCE CSE\\Desktop\\Instruct\\myenv\\Lib\\site-packages\\torch\\utils\\checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7538, 'grad_norm': 1.5464049577713013, 'learning_rate': 0.0002, 'epoch': 2.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\BMSCE CSE\\Desktop\\Instruct\\myenv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\BMSCE CSE\\Desktop\\Instruct\\myenv\\Lib\\site-packages\\torch\\utils\\checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7456, 'grad_norm': 1.5383796691894531, 'learning_rate': 0.0002, 'epoch': 2.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\BMSCE CSE\\Desktop\\Instruct\\myenv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\BMSCE CSE\\Desktop\\Instruct\\myenv\\Lib\\site-packages\\torch\\utils\\checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6747, 'grad_norm': 1.6993037462234497, 'learning_rate': 0.0002, 'epoch': 3.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\BMSCE CSE\\Desktop\\Instruct\\myenv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\BMSCE CSE\\Desktop\\Instruct\\myenv\\Lib\\site-packages\\torch\\utils\\checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.562, 'grad_norm': 1.9063868522644043, 'learning_rate': 0.0002, 'epoch': 3.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\BMSCE CSE\\Desktop\\Instruct\\myenv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\BMSCE CSE\\Desktop\\Instruct\\myenv\\Lib\\site-packages\\torch\\utils\\checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5546, 'grad_norm': 1.8105595111846924, 'learning_rate': 0.0002, 'epoch': 3.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\BMSCE CSE\\Desktop\\Instruct\\myenv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\BMSCE CSE\\Desktop\\Instruct\\myenv\\Lib\\site-packages\\torch\\utils\\checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5915, 'grad_norm': 1.9690666198730469, 'learning_rate': 0.0002, 'epoch': 3.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\BMSCE CSE\\Desktop\\Instruct\\myenv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\BMSCE CSE\\Desktop\\Instruct\\myenv\\Lib\\site-packages\\torch\\utils\\checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5573, 'grad_norm': 2.801684617996216, 'learning_rate': 0.0002, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\BMSCE CSE\\Desktop\\Instruct\\myenv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\BMSCE CSE\\Desktop\\Instruct\\myenv\\Lib\\site-packages\\torch\\utils\\checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4824, 'grad_norm': 2.9559481143951416, 'learning_rate': 0.0002, 'epoch': 4.24}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\BMSCE CSE\\Desktop\\Instruct\\myenv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\BMSCE CSE\\Desktop\\Instruct\\myenv\\Lib\\site-packages\\torch\\utils\\checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4147, 'grad_norm': 2.671727418899536, 'learning_rate': 0.0002, 'epoch': 4.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\BMSCE CSE\\Desktop\\Instruct\\myenv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\BMSCE CSE\\Desktop\\Instruct\\myenv\\Lib\\site-packages\\torch\\utils\\checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4481, 'grad_norm': 3.120180130004883, 'learning_rate': 0.0002, 'epoch': 4.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\BMSCE CSE\\Desktop\\Instruct\\myenv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 473.116, 'train_samples_per_second': 6.764, 'train_steps_per_second': 0.423, 'train_loss': 1.8956913948059082, 'epoch': 4.71}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=200, training_loss=1.8956913948059082, metrics={'train_runtime': 473.116, 'train_samples_per_second': 6.764, 'train_steps_per_second': 0.423, 'total_flos': 6241422744357888.0, 'train_loss': 1.8956913948059082, 'epoch': 4.705882352941177})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.use_cache = False\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\BMSCE CSE\\Desktop\\Instruct\\myenv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7be5a8623ea430bb95d7e15cc46d690",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64d103d309ec44d39b5456687d3f6290",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/4.98k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02217a6e8aaa4d0c9f605380ab527a33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/37.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Aditi25/Instruct_70_30_split/commit/889a441964135249cc1eb86fef833a6726fff9c6', commit_message='End of training', commit_description='', oid='889a441964135249cc1eb86fef833a6726fff9c6', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.save_pretrained(\"trained-model\")\n",
    "# model.push_to_hub(\n",
    "#     \"Aditi25/experimenting_with_falcon_instruct\", use_auth_token=True\n",
    "# )\n",
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4fb1577db2c49e3ba50b7f7a021450d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/673 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\BMSCE CSE\\Desktop\\Instruct\\myenv\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\BMSCE CSE\\.cache\\huggingface\\hub\\models--Aditi25--Instruct_70_30_split. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "\n",
      "WARNING: You are currently loading Falcon using legacy code contained in the model repository. Falcon has now been fully ported into the Hugging Face transformers library. For the most up-to-date and high-performance version of the Falcon model code, please update to the latest version of transformers and then load the model without the trust_remote_code=True argument.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f35416cfd1764f1791e0bc7e2d1594f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44b3b255f7bf4087aad0671c23647f23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/37.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "PEFT_MODEL = \"Aditi25/Instruct_70_30_split\"\n",
    "\n",
    "config = PeftConfig.from_pretrained(PEFT_MODEL)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    return_dict=True,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = PeftModel.from_pretrained(model, PEFT_MODEL)\n",
    "\n",
    "# PEFT_MODEL = \"Aditi25/experimenting_with_falcon_instruct\"\n",
    "\n",
    "# config = PeftConfig.from_pretrained(PEFT_MODEL)\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     config.base_model_name_or_path,\n",
    "#     return_dict=True,\n",
    "#     # quantization_config=bnb_config,\n",
    "#     device_map= \"auto\",\n",
    "#     trust_remote_code=True,\n",
    "# )\n",
    "# tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "\n",
    "# model = PeftModel.from_pretrained(model, PEFT_MODEL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PEFT_MODEL: Aditi25/Instruct_70_30_split\n"
     ]
    }
   ],
   "source": [
    "print(\"PEFT_MODEL:\", PEFT_MODEL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(question: str) -> str:\n",
    "    prompt = f\"\"\"\n",
    "<human>: {question}\n",
    "<assistant>:\n",
    "\"\"\".strip()\n",
    "    # encoding = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "    encoding = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    with torch.inference_mode():\n",
    "        outputs = model.generate(\n",
    "            input_ids=encoding.input_ids,\n",
    "            attention_mask=encoding.attention_mask,\n",
    "            generation_config=generation_config,\n",
    "        )\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    assistant_start = \"<assistant>:\"\n",
    "    response_start = response.find(assistant_start)\n",
    "    return response[response_start + len(assistant_start) :].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping malformed text: ### Human 2: What lines should be added at the end of the /etc/chrony.conf file for NTP broadcasting? ### Assistant 2: Add lines for NTP server identification and NTP broadcasting to the management network and BMC network.\n",
      "Skipping malformed text: ### Human 4: What network configurations are defined in the [templates] section of the cluster definition file example?   ### Assistant 4: The [templates] section defines network configurations like mgmt_net_name, mgmt_bmc_net_name, mgmt_net_interfaces, and more for the compute nodes.  \n",
      "Skipping malformed text: ### Human 14: How can you confirm the availability of free space on the initialized disk devices? ### Assistant 14: By examining the output of the pvscan command, you can determine the amount of free space available on the initialized disk devices.\n",
      "Skipping malformed text: ### Human 20: How are the compute nodes mapped to specific templates in the cluster definition file example?   ### Assistant 20: Compute nodes are mapped to specific templates based on their internal_name and associated network configurations within the cluster definition file.\n",
      "Skipping malformed text: ### Human 9:What should be done when splitting a single configuration file into multiple files according to the text? ### Assistant 9:When splitting a single configuration file, copy the original file, name the files according to the components they describe, retain lines related to those components, and delete irrelevant lines.\n",
      "Skipping malformed text: ### Human 1: How should you prepare to run YaST from a PuTTY window simultaneously with the cluster manager installation? ### Assistant 1: To run YaST from a PuTTY window, you should enter the command: `export NCURSES_NO_UTF8_ACS=1`.\n",
      "Skipping malformed text: ### Human 20: Why is it important to choose a disk recognized by both physical nodes for a common disk? ### Assistant 20: Selecting a disk recognized by both physical nodes ensures seamless data exchange and compatibility between the nodes when operating in a clustered environment.\n",
      "Skipping malformed text: ### Human 1: What is the purpose of the cluster definition file mentioned in the text?   ### Assistant 1: The cluster definition file specifies configuration details for compute nodes with specific architectures and additional information for Arm (AArch64) architecture nodes.  \n",
      "Skipping malformed text: ### Human 5: How can you display a list of time zones on the admin node? ### Assistant 5: By entering the command \"# timedatectl list-timezones\" on the admin node.\n",
      "Skipping malformed text: ### Human 2: What steps should be completed on the Language and Keyboard Layout screen during the operating system installation? ### Assistant 2: On the Language and Keyboard Layout screen, you should select your language, select your keyboard layout, and then select Next.\n",
      "Skipping malformed text: ### Human 5:Why is it recommended to create more than one cluster definition file for some clusters? ### Assistant 5:Creating more than one cluster definition file allows for a step-by-step approach in configuring the components into the cluster in the desired order.\n",
      "Skipping malformed text: ### Human 1: What devices are listed in the provided output that belong to the HPE storage module? ### Assistant 1: The devices listed in the output are storage HPE P408i-a SR Gen10 1.04 and enclosures HP MSA 2050 SAS G22x with corresponding disk IDs.\n",
      "Skipping malformed text: ### Human 8: How can you direct network time protocol (NTP) server requests to the server at your site instead of public time servers? ### Assistant 8: Complete the steps outlined to direct NTP server requests to your site's server.\n",
      "Skipping malformed text: ### Human 20: How can you verify the time zone information after configuring it on the admin node? ### Assistant 20: By using the timedatectl command to display the configured time zone information.\n",
      "Skipping malformed text: ### Human 7: How can one verify if the static hostname is correctly set on an admin node using the hostnamectl command?   ### Assistant 7: Use the command: hostnamectl  \n",
      "Skipping malformed text: ### Human 8: What can you determine using the \"ip addr show\" command? ### Assistant 8: The \"ip addr show\" command allows you to determine the NIC name and MAC address connecting the admin node to the house network.\n",
      "Skipping malformed text: ### Human 4: What steps are involved in modifying the /etc/chrony.conf file for NTP server configuration? ### Assistant 4: Use a text editor to open the file, modify the NTP server lines, add NTP broadcasting lines, and save/close the file.\n",
      "Skipping malformed text: ### Human 19: What security provisions are made for the management BMC network access in the cluster definition file example?   ### Assistant 19: The cluster definition file includes provisions for BMC network access security with specified usernames and passwords for managing the compute nodes.  \n",
      "Skipping malformed text: ### Human 3: What actions need to be taken on the License Agreement screen for the operating system installation? ### Assistant 3: On the License Agreement screen, you should tab to the box labeled with \"[ ] I Agree ...\", press the space bar to accept the license terms, and then select Next.\n",
      "Skipping malformed text: ### Human 3: What is recommended by Hewlett Packard Enterprise regarding password storage practices for switch configuration?   ### Assistant 3: Hewlett Packard Enterprise strongly recommends implementing standard and secure practices to store all passwords securely and not lose this information.\n",
      "Skipping malformed text: ### Human 2: What command is used to change the management switch password for the admin account?   ### Assistant 2: Use the cm mgmtswitch set command with the specified format: cm mgmtswitch set -s hostname -p new_password --update-switch --skip-update-config.\n",
      "Skipping malformed text: ### Human 3:How should the cluster definition file be split for the clusters mentioned? ### Assistant 3:The cluster definition file should be split into two files: one for the management switches and one for the compute nodes.\n",
      "Skipping malformed text: ### Human 5: In the given example environment, which disk is considered a safe choice for the common disk? ### Assistant 5: In the example environment, /dev/sdc is recommended as a safe choice for the common disk since it is recognized by both physical nodes and is not in use.\n",
      "Skipping malformed text: ### Human 9: What transport method is defined for the network in the cluster definition file example?   ### Assistant 9: The transport method specified in the cluster definition file is 'udpcast' for the management network of the compute nodes.  \n",
      "Skipping malformed text: ### Human 2: What lines should be added for a SAC-HA configuration with two physical admin nodes? ### Assistant 2: Physical node addresses for a SAC-HA configuration with two admin nodes should be added.\n",
      "Skipping malformed text: ### Human 14: Should the admin node hostname be the short name or the fully qualified domain name? ### Assistant 14: The admin node hostname should be the short name, not the fully qualified domain name.\n",
      "Skipping malformed text: ### Human 5: What is the purpose of the [nic_templates] section in the cluster definition file example?   ### Assistant 5: The [nic_templates] section specifies network templates for the compute nodes regarding network interfaces, bonding, and networks like ib0 and ib1.  \n",
      "Skipping malformed text: ### Human 11: How can you ensure the correct time zone is set for the admin node? ### Assistant 11: By selecting one of the time zones from the timedatectl list-timezones command output and setting it using timedatectl.\n",
      "Skipping malformed text: ### Human 8: How can you find a disk that both nodes can recognize and that is not currently in use? ### Assistant 8: By analyzing the output of lsscsi and pvscan commands, you can identify a disk that is recognized by both nodes and is not currently in use.\n",
      "Skipping malformed text: ### Human 10: How is network redundancy handled in the cluster definition file example?   ### Assistant 10: The cluster definition file specifies 'redundant_mgmt_network' as a parameter to handle network redundancy in the cluster setup.  \n",
      "Skipping malformed text: ### Human 1: How do you monitor the switch configuration process?   ### Assistant 1: Use the tail -f /opt/clmgr/log/switchconfig.log command to monitor the progress of the switch configuration.\n",
      "Skipping malformed text: ### Human 13: What disk configuration is defined for the compute nodes in the cluster file example?   ### Assistant 13: The root file system (rootfs) configuration specified for the compute nodes is 'disk' in the cluster definition file example.  \n",
      "Skipping malformed text: ### Human 1: How can you modify the /etc/chrony.conf file for NTP server identification? ### Assistant 1: Insert a pound character (#) in column 1 of each line containing rhel.pool.ntp.org.\n",
      "Skipping malformed text: ### Human 7:In cases where some compute nodes serve as service or login nodes, what additional file should be created? ### Assistant 7:An additional cluster definition file should be created for those compute nodes that serve as service or login nodes.\n",
      "Skipping malformed text: ### Human 10: What information does the timedatectl command display about the admin node time zone? ### Assistant 10: It displays local time, universal time, RTC time, time zone, NTP status, DST status, and last/next DST changes.\n",
      "Skipping malformed text: ### Human 18: How is the template name 'compute' utilized in the cluster definition file example?   ### Assistant 18: The template name 'compute' is used to define various network and configuration parameters specific to the compute nodes in the cluster setup.  \n",
      "Skipping malformed text: ### Human 8: For compute nodes in the cluster, what authentication credentials are specified in the example cluster file?   ### Assistant 8: The cluster definition file includes credentials such as the bmc_username and bmc_password for accessing and managing the compute nodes.  \n",
      "Skipping malformed text: ### Human 7: How are compute nodes uniquely identified in the cluster definition file example?   ### Assistant 7: Compute nodes are identified by their internal_name and network MAC addresses (mgmt_bmc_net_macs and mgmt_net_macs) within the cluster definition file.  \n",
      "Skipping malformed text: ### Human 8: How can you monitor the PXE boot process on one or more compute nodes?   ### Assistant 8: Use the cm node console command with the format: cm node console -n hostname.\n",
      "Skipping malformed text: ### Human 13: What actions can be taken as an alternative to prevent data loss when using disks recognized in the pvscan output? ### Assistant 13: To prevent data loss on disks recognized in the pvscan output, consider moving the existing data to another disk before proceeding with the HA admin node setup.\n",
      "Skipping malformed text: ### Human 6: What details are provided in the [discover] section of the cluster definition file example?   ### Assistant 6: The [discover] section includes details about network devices like switches (mgmtsw0, mgmtsw1) and service nodes (service1 to service6) with their respective network configurations.  \n",
      "Skipping malformed text: ### Human 3: How can the time zone be verified on the admin nodes?   ### Assistant 3: Enter the command: # date  \n",
      "Skipping malformed text: ### Human 3: How should you configure three physical admin nodes for a quorum HA configuration? ### Assistant 3: Lines for physical node addresses in a quorum HA configuration with three admin nodes should be added.\n",
      "Skipping malformed text: ### Human 3: Can you provide an example of a cluster definition file for AArch64 architecture compute nodes?   ### Assistant 3: A file named aarch64_compute.config is provided in the text as an example of a cluster definition file for AArch64 architecture compute nodes.  \n",
      "Skipping malformed text: ### Human 2: What keywords should be specified for compute nodes with Arm (AArch64) architecture type in the cluster file?   ### Assistant 2: The keywords to specify for Arm (AArch64) architecture nodes include image, kernel, and architecture.  \n",
      "Skipping malformed text: ### Human 4: What should you do if the time zone information on the cluster is incorrect? ### Assistant 4: You should enter the command \"timedatectl set-timezone time_zone\" to set the correct time zone.\n",
      "Skipping malformed text: ### Human 11: What type of disks should be considered for use with caution when the HA admin node starts running? ### Assistant 11: Disks that contain data and appear in the pvscan output should be used with caution, as data stored on those disks may be destroyed during the HA admin node operation.\n",
      "Skipping malformed text: ### Human 12: How can you confirm that a disk is recognized by both physical nodes? ### Assistant 12: By checking the output of commands like lsscsi and pvscan on both physical nodes, you can verify if a disk is recognized by both nodes.\n",
      "Skipping malformed text: ### Human 4: What command should be used to verify the hostname and IP address on the admin nodes?   ### Assistant 4: Enter the command: # cat /etc/hosts  \n",
      "Skipping malformed text: ### Human 10: What is the significance of the World Wide Name (WWN) in identifying disks? ### Assistant 10: WWN provides a unique identifier for disks, which is crucial for accurately selecting and differentiating between disks in a system.\n",
      "Skipping malformed text: ### Human 11: When should the physical admin nodes communicate with each other be verified?   ### Assistant 11: Verify that physical admin nodes can communicate with each other if they are configured for high availability (HA).  \n",
      "Skipping malformed text: ### Human 4: How can you save the changed configuration to the nonvolatile memory on the switches?   ### Assistant 4: Enter the command: switchconfig config -s all --save to save the changed configuration to the nonvolatile memory on the switches.\n",
      "Skipping malformed text: ### Human 13: When should the hostname be set on the admin node? ### Assistant 13: The hostname should be set using hostnamectl command after completing other configuration steps.\n",
      "Skipping malformed text: ### Human 2: What steps should be completed to verify the configuration on each physical admin node as the root user?   ### Assistant 2: Log into each physical admin node as the root user and follow the steps provided in the procedure.  \n",
      "Skipping malformed text: ### Human 8: When should IOMMU be verified as enabled?   ### Assistant 8: Verify that IOMMU is enabled if you have completed the procedure for enabling an input-output memory management unit (IOMMU).  \n",
      "Skipping malformed text: ### Human 15: What is the baud rate specified for the console communication in the cluster definition file example?   ### Assistant 15: The baud rate defined for the console communication in the cluster file is '115200' for the console device 'ttyS0'.  \n",
      "Skipping malformed text: ### Human 1: How can you install the operating system and the cluster manager simultaneously on the admin node? ### Assistant 1: You can use the intelligent platform management interface (IPMI) tool or attach your own KVM equipment to the cluster.\n",
      "Skipping malformed text: ### Human 5: How can the time be verified on the admin nodes?   ### Assistant 5: Enter the command: # chronyc sources -v  \n",
      "Skipping malformed text: ### Human 15: Why is it essential to choose a disk that is not currently in use for the HA admin node? ### Assistant 15: Selecting a disk that is not currently in use ensures that existing data on the disk is not compromised or lost when the HA admin node starts operating.\n",
      "Skipping malformed text: ### Human 4: How should you specify the NIC information on the Network Settings screen during installation? ### Assistant 4: To specify the NIC information, you should highlight the NIC with the lowest MAC address.\n",
      "Skipping malformed text: ### Human 8:What are the example file names provided for cluster definition files? ### Assistant 8:Example file names include mgmtsw.config for management switches only and compute.config for compute nodes.\n",
      "Skipping malformed text: ### Human 4: What needs to be done to save and close the /etc/hosts file? ### Assistant 4: To save and close the /etc/hosts file, ensure proper saving and closing actions are performed.\n",
      "Skipping malformed text: ### Human 12: What information should be specified when setting the time zone using timedatectl? ### Assistant 12: You need to specify one of the time zones from the list-timezones command output.\n",
      "Skipping malformed text: ### Human 10: What output indicates that IOMMU is enabled on a properly configured system?   ### Assistant 10: The output should show: [0.000000] DMAR: IOMMU enabled  \n",
      "Skipping malformed text: ### Human 12: What type of console device is configured for the compute nodes in the cluster definition file example?   ### Assistant 12: The console device configured for the compute nodes in the cluster file is 'ttyS0' as mentioned in the cluster definition file.  \n",
      "Skipping malformed text: ### Human 7: What command should be used to determine network interface card details? ### Assistant 7: The command \"ip addr show\" should be used to determine network interface card details.\n",
      "Skipping malformed text: ### Human 2: What steps should be completed to configure a high availability admin node? ### Assistant 2: Complete the necessary steps on all physical admin nodes.\n",
      "Skipping malformed text: ### Human 17: In the cluster definition file example, what additional software is mentioned for the compute nodes with AArch64 architecture?   ### Assistant 17: The cluster definition file mentions specific software versions like 'sles15sp5-arm64' for the image and '5.14.21-150500.48-default' for the kernel of the AArch64 architecture compute nodes.  \n",
      "Skipping malformed text: ### Human 17: What is the purpose of setting the time zone on the admin node? ### Assistant 17: Setting the time zone ensures accurate timekeeping and synchronization on the admin node.\n",
      "Skipping malformed text: ### Human 3: What precaution is advised when selecting a disk that both nodes can recognize and is not in current use? ### Assistant 3: Avoid selecting a disk that appears in the pvscan output, as it likely already contains data. Choose a disk that is recognized by both nodes and is not currently in use to prevent data loss.\n",
      "Skipping malformed text: ### Human 9: What command should be entered to check if IOMMU is enabled?   ### Assistant 9: Enter the command: # dmesg | grep -E \"DMAR: IOMMU\"  \n",
      "Skipping malformed text: ### Human 5: How should entries be added in the /etc/hosts file for each physical admin node? ### Assistant 5: Add entries with the network address, FQDN, and hostname of each physical admin node in a specific format.\n",
      "Skipping malformed text: ### Human 16: How can you ensure correct NTP server requests are directed on the admin node? ### Assistant 16: Follow the steps provided to direct NTP server requests to the server at your site.\n",
      "Skipping malformed text: ### Human 15: What information should be specified when configuring an HA admin node? ### Assistant 15: Specify the admin_node_hostname of the node being configured at that time.\n",
      "Skipping malformed text: ### Human 2:What information is contained in the original cluster definition file for the mentioned clusters? ### Assistant 2:The original cluster definition file contains information necessary to configure the management switches and the compute nodes.\n",
      "Skipping malformed text: ### Human 16: How are the templates differentiated in the cluster definition file example?   ### Assistant 16: Templates are differentiated based on network assignments, interfaces, and bonding configurations for compute nodes in the cluster definition file.  \n",
      "Skipping malformed text: ### Human 6: Provide an example of adding information to the /etc/resolv.conf file. ### Assistant 6: An example of adding information to the /etc/resolv.conf file involves including nameserver and search keywords with relevant addresses.\n",
      "Skipping malformed text: ### Human 7: What is the recommended way to enter the hostname for the admin node? ### Assistant 7: You should enter the short name of the hostname, not the fully qualified domain name (FQDN).\n",
      "Skipping malformed text: ### Human 17: How can you prevent potential data loss when selecting a disk recognized by both nodes? ### Assistant 17: To prevent data loss, move any existing data on the disk to another location before initializing it for use by both nodes to maintain data integrity.\n",
      "Skipping malformed text: ### Human 1:What type of cluster examples are mentioned for splitting the cluster definition file? ### Assistant 1:HPE Cray XD or HPE Apollo clusters without SU leader nodes, such as an HPE Apollo 80 or an HPE Apollo 2000.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating responses:   0%|          | 0/597 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\BMSCE CSE\\Desktop\\Instruct\\myenv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:535: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:   0%|          | 1/597 [00:01<17:28,  1.76s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:   0%|          | 2/597 [00:04<23:28,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:   1%|          | 3/597 [00:05<19:07,  1.93s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:   1%|          | 4/597 [00:06<13:12,  1.34s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:   1%|          | 5/597 [00:08<16:19,  1.65s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:   1%|          | 6/597 [00:10<16:37,  1.69s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:   1%|          | 7/597 [00:11<15:59,  1.63s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:   1%|▏         | 8/597 [00:13<14:59,  1.53s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:   2%|▏         | 9/597 [00:15<17:13,  1.76s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:   2%|▏         | 10/597 [00:16<15:04,  1.54s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:   2%|▏         | 11/597 [00:18<16:56,  1.73s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:   2%|▏         | 12/597 [00:19<15:37,  1.60s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:   2%|▏         | 13/597 [00:21<13:58,  1.44s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:   2%|▏         | 14/597 [00:23<17:06,  1.76s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:   3%|▎         | 15/597 [00:25<16:41,  1.72s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:   3%|▎         | 16/597 [00:26<15:20,  1.58s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:   3%|▎         | 17/597 [00:28<16:13,  1.68s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:   3%|▎         | 18/597 [00:31<21:12,  2.20s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:   3%|▎         | 19/597 [00:33<18:31,  1.92s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:   3%|▎         | 20/597 [00:34<17:03,  1.77s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:   4%|▎         | 21/597 [00:35<15:32,  1.62s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:   4%|▎         | 22/597 [00:36<13:29,  1.41s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:   4%|▍         | 23/597 [00:37<12:02,  1.26s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:   4%|▍         | 24/597 [00:39<14:34,  1.53s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:   4%|▍         | 25/597 [00:40<13:04,  1.37s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:   4%|▍         | 26/597 [00:42<13:28,  1.42s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:   5%|▍         | 27/597 [00:44<15:55,  1.68s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:   5%|▍         | 28/597 [00:45<15:09,  1.60s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:   5%|▍         | 29/597 [00:47<14:18,  1.51s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:   5%|▌         | 30/597 [00:48<13:33,  1.43s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:   5%|▌         | 31/597 [00:52<19:47,  2.10s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:   5%|▌         | 32/597 [00:53<18:45,  1.99s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:   6%|▌         | 33/597 [00:55<18:56,  2.01s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:   6%|▌         | 34/597 [00:57<17:46,  1.90s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:   6%|▌         | 35/597 [00:59<18:18,  1.95s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:   6%|▌         | 36/597 [01:00<15:28,  1.66s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:   6%|▌         | 37/597 [01:02<16:12,  1.74s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:   6%|▋         | 38/597 [01:04<16:29,  1.77s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:   7%|▋         | 39/597 [01:06<17:40,  1.90s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:   7%|▋         | 40/597 [01:07<15:28,  1.67s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:   7%|▋         | 41/597 [01:09<15:11,  1.64s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:   7%|▋         | 42/597 [01:10<14:37,  1.58s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:   7%|▋         | 43/597 [01:11<12:44,  1.38s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:   7%|▋         | 44/597 [01:12<11:34,  1.26s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:   8%|▊         | 45/597 [01:13<11:26,  1.24s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:   8%|▊         | 46/597 [01:15<13:34,  1.48s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:   8%|▊         | 47/597 [01:17<13:21,  1.46s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:   8%|▊         | 48/597 [01:18<12:56,  1.41s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:   8%|▊         | 49/597 [01:19<12:54,  1.41s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:   8%|▊         | 50/597 [01:21<12:13,  1.34s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:   9%|▊         | 51/597 [01:22<12:57,  1.42s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:   9%|▊         | 52/597 [01:23<11:02,  1.21s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:   9%|▉         | 53/597 [01:24<09:31,  1.05s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:   9%|▉         | 54/597 [01:24<08:29,  1.07it/s]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:   9%|▉         | 55/597 [01:26<10:42,  1.18s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:   9%|▉         | 56/597 [01:28<12:57,  1.44s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  10%|▉         | 57/597 [01:30<13:59,  1.55s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  10%|▉         | 58/597 [01:31<12:06,  1.35s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  10%|▉         | 59/597 [01:33<15:32,  1.73s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  10%|█         | 60/597 [01:36<16:33,  1.85s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  10%|█         | 61/597 [01:37<14:42,  1.65s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  10%|█         | 62/597 [01:39<17:39,  1.98s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  11%|█         | 63/597 [01:41<17:02,  1.92s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  11%|█         | 64/597 [01:43<17:40,  1.99s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  11%|█         | 65/597 [01:45<16:20,  1.84s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  11%|█         | 66/597 [01:46<15:07,  1.71s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  11%|█         | 67/597 [01:47<13:16,  1.50s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  11%|█▏        | 68/597 [01:49<14:54,  1.69s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  12%|█▏        | 69/597 [01:54<21:04,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  12%|█▏        | 70/597 [01:55<18:30,  2.11s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  12%|█▏        | 71/597 [01:56<16:02,  1.83s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  12%|█▏        | 72/597 [01:58<15:43,  1.80s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  12%|█▏        | 73/597 [01:59<13:48,  1.58s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  12%|█▏        | 74/597 [02:02<16:25,  1.88s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  13%|█▎        | 75/597 [02:03<14:22,  1.65s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  13%|█▎        | 76/597 [02:04<13:05,  1.51s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  13%|█▎        | 77/597 [02:05<12:25,  1.43s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  13%|█▎        | 78/597 [02:06<11:02,  1.28s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  13%|█▎        | 79/597 [02:07<10:28,  1.21s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  13%|█▎        | 80/597 [02:08<09:49,  1.14s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  14%|█▎        | 81/597 [02:09<09:38,  1.12s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  14%|█▎        | 82/597 [02:10<10:17,  1.20s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  14%|█▍        | 83/597 [02:12<09:54,  1.16s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  14%|█▍        | 84/597 [02:13<10:04,  1.18s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  14%|█▍        | 85/597 [02:14<09:21,  1.10s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  14%|█▍        | 86/597 [02:15<08:46,  1.03s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  15%|█▍        | 87/597 [02:16<09:18,  1.10s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  15%|█▍        | 88/597 [02:17<10:14,  1.21s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  15%|█▍        | 89/597 [02:19<10:46,  1.27s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  15%|█▌        | 90/597 [02:20<10:25,  1.23s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  15%|█▌        | 91/597 [02:21<11:02,  1.31s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  15%|█▌        | 92/597 [02:23<12:47,  1.52s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  16%|█▌        | 93/597 [02:24<11:45,  1.40s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  16%|█▌        | 94/597 [02:28<17:30,  2.09s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  16%|█▌        | 95/597 [02:29<14:39,  1.75s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  16%|█▌        | 96/597 [02:30<12:17,  1.47s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  16%|█▌        | 97/597 [02:31<11:02,  1.33s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  16%|█▋        | 98/597 [02:32<09:14,  1.11s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  17%|█▋        | 99/597 [02:33<09:58,  1.20s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  17%|█▋        | 100/597 [02:34<09:42,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  17%|█▋        | 101/597 [02:36<10:32,  1.27s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  17%|█▋        | 102/597 [02:37<11:19,  1.37s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  17%|█▋        | 103/597 [02:39<11:44,  1.43s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  17%|█▋        | 104/597 [02:41<13:16,  1.61s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  18%|█▊        | 105/597 [02:42<12:58,  1.58s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  18%|█▊        | 106/597 [02:43<10:27,  1.28s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  18%|█▊        | 107/597 [02:44<10:16,  1.26s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  18%|█▊        | 108/597 [02:45<09:39,  1.19s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  18%|█▊        | 109/597 [02:46<09:50,  1.21s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  18%|█▊        | 110/597 [02:50<16:00,  1.97s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  19%|█▊        | 111/597 [02:52<15:04,  1.86s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  19%|█▉        | 112/597 [02:53<14:31,  1.80s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  19%|█▉        | 113/597 [02:55<13:12,  1.64s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  19%|█▉        | 114/597 [02:55<11:18,  1.40s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  19%|█▉        | 115/597 [02:57<11:28,  1.43s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  19%|█▉        | 116/597 [02:58<10:50,  1.35s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  20%|█▉        | 117/597 [03:00<12:17,  1.54s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  20%|█▉        | 118/597 [03:02<12:08,  1.52s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  20%|█▉        | 119/597 [03:03<11:16,  1.42s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  20%|██        | 120/597 [03:04<11:24,  1.44s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  20%|██        | 121/597 [03:05<10:02,  1.27s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  20%|██        | 122/597 [03:06<09:28,  1.20s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  21%|██        | 123/597 [03:07<07:45,  1.02it/s]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  21%|██        | 124/597 [03:08<09:50,  1.25s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  21%|██        | 125/597 [03:10<09:31,  1.21s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  21%|██        | 126/597 [03:10<08:14,  1.05s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  21%|██▏       | 127/597 [03:11<08:29,  1.08s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  21%|██▏       | 128/597 [03:13<10:01,  1.28s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  22%|██▏       | 129/597 [03:14<09:57,  1.28s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  22%|██▏       | 130/597 [03:16<11:37,  1.49s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  22%|██▏       | 131/597 [03:17<10:38,  1.37s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  22%|██▏       | 132/597 [03:19<11:13,  1.45s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  22%|██▏       | 133/597 [03:20<09:45,  1.26s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  22%|██▏       | 134/597 [03:21<08:28,  1.10s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  23%|██▎       | 135/597 [03:22<08:22,  1.09s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  23%|██▎       | 136/597 [03:23<09:36,  1.25s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  23%|██▎       | 137/597 [03:25<09:59,  1.30s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  23%|██▎       | 138/597 [03:26<09:02,  1.18s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  23%|██▎       | 139/597 [03:27<10:23,  1.36s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  23%|██▎       | 140/597 [03:29<09:53,  1.30s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  24%|██▎       | 141/597 [03:31<11:27,  1.51s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  24%|██▍       | 142/597 [03:33<14:11,  1.87s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  24%|██▍       | 143/597 [03:34<12:08,  1.60s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  24%|██▍       | 144/597 [03:35<10:46,  1.43s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  24%|██▍       | 145/597 [03:36<09:50,  1.31s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  24%|██▍       | 146/597 [03:38<09:29,  1.26s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  25%|██▍       | 147/597 [03:39<09:16,  1.24s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  25%|██▍       | 148/597 [03:39<08:13,  1.10s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  25%|██▍       | 149/597 [03:41<08:35,  1.15s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  25%|██▌       | 150/597 [03:42<08:26,  1.13s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  25%|██▌       | 151/597 [03:42<07:10,  1.04it/s]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  25%|██▌       | 152/597 [03:43<06:56,  1.07it/s]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  26%|██▌       | 153/597 [03:45<07:41,  1.04s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  26%|██▌       | 154/597 [03:49<14:20,  1.94s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  26%|██▌       | 155/597 [03:50<12:17,  1.67s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  26%|██▌       | 156/597 [03:50<10:27,  1.42s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  26%|██▋       | 157/597 [03:52<11:05,  1.51s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  26%|██▋       | 158/597 [03:53<10:19,  1.41s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  27%|██▋       | 159/597 [03:54<09:19,  1.28s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  27%|██▋       | 160/597 [03:56<09:15,  1.27s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  27%|██▋       | 161/597 [03:57<09:02,  1.24s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  27%|██▋       | 162/597 [03:58<09:17,  1.28s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  27%|██▋       | 163/597 [04:00<09:48,  1.36s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  27%|██▋       | 164/597 [04:01<09:56,  1.38s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  28%|██▊       | 165/597 [04:02<08:49,  1.23s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  28%|██▊       | 166/597 [04:04<09:46,  1.36s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  28%|██▊       | 167/597 [04:05<09:20,  1.30s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  28%|██▊       | 168/597 [04:06<08:18,  1.16s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  28%|██▊       | 169/597 [04:07<09:41,  1.36s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  28%|██▊       | 170/597 [04:09<09:01,  1.27s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  29%|██▊       | 171/597 [04:10<09:35,  1.35s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  29%|██▉       | 172/597 [04:11<08:46,  1.24s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  29%|██▉       | 173/597 [04:12<08:43,  1.23s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  29%|██▉       | 174/597 [04:15<11:10,  1.59s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  29%|██▉       | 175/597 [04:15<09:05,  1.29s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  29%|██▉       | 176/597 [04:16<08:35,  1.22s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  30%|██▉       | 177/597 [04:17<07:55,  1.13s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  30%|██▉       | 178/597 [04:18<07:26,  1.07s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  30%|██▉       | 179/597 [04:19<06:47,  1.02it/s]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  30%|███       | 180/597 [04:20<06:57,  1.00s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  30%|███       | 181/597 [04:21<06:39,  1.04it/s]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  30%|███       | 182/597 [04:22<07:14,  1.05s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  31%|███       | 183/597 [04:24<07:59,  1.16s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  31%|███       | 184/597 [04:24<07:09,  1.04s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  31%|███       | 185/597 [04:25<06:16,  1.09it/s]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  31%|███       | 186/597 [04:26<06:27,  1.06it/s]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  31%|███▏      | 187/597 [04:27<06:59,  1.02s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  31%|███▏      | 188/597 [04:29<07:55,  1.16s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  32%|███▏      | 189/597 [04:30<08:29,  1.25s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  32%|███▏      | 190/597 [04:31<07:53,  1.16s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  32%|███▏      | 191/597 [04:32<06:51,  1.01s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  32%|███▏      | 192/597 [04:34<09:06,  1.35s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  32%|███▏      | 193/597 [04:35<08:36,  1.28s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  32%|███▏      | 194/597 [04:36<08:01,  1.20s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  33%|███▎      | 195/597 [04:37<06:51,  1.02s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  33%|███▎      | 196/597 [04:38<07:59,  1.20s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  33%|███▎      | 197/597 [04:39<08:11,  1.23s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  33%|███▎      | 198/597 [04:42<10:23,  1.56s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  33%|███▎      | 199/597 [04:43<10:21,  1.56s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  34%|███▎      | 200/597 [04:45<10:51,  1.64s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  34%|███▎      | 201/597 [04:46<10:04,  1.53s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  34%|███▍      | 202/597 [04:48<09:26,  1.44s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  34%|███▍      | 203/597 [04:49<08:17,  1.26s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  34%|███▍      | 204/597 [04:50<08:13,  1.25s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  34%|███▍      | 205/597 [04:51<08:58,  1.37s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  35%|███▍      | 206/597 [04:52<07:34,  1.16s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  35%|███▍      | 207/597 [04:53<07:22,  1.13s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  35%|███▍      | 208/597 [04:55<08:36,  1.33s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  35%|███▌      | 209/597 [04:56<08:32,  1.32s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  35%|███▌      | 210/597 [04:57<07:33,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  35%|███▌      | 211/597 [04:59<08:05,  1.26s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  36%|███▌      | 212/597 [05:00<08:06,  1.26s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  36%|███▌      | 213/597 [05:01<07:30,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  36%|███▌      | 214/597 [05:02<08:08,  1.27s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  36%|███▌      | 215/597 [05:03<07:48,  1.23s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  36%|███▌      | 216/597 [05:04<06:20,  1.00it/s]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  36%|███▋      | 217/597 [05:05<06:54,  1.09s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  37%|███▋      | 218/597 [05:06<06:26,  1.02s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  37%|███▋      | 219/597 [05:07<06:41,  1.06s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  37%|███▋      | 220/597 [05:09<08:39,  1.38s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  37%|███▋      | 221/597 [05:11<08:26,  1.35s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  37%|███▋      | 222/597 [05:13<10:05,  1.61s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  37%|███▋      | 223/597 [05:14<09:18,  1.49s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  38%|███▊      | 224/597 [05:15<08:12,  1.32s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  38%|███▊      | 225/597 [05:17<10:00,  1.62s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  38%|███▊      | 226/597 [05:18<08:19,  1.35s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  38%|███▊      | 227/597 [05:19<07:18,  1.18s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  38%|███▊      | 228/597 [05:21<08:18,  1.35s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  38%|███▊      | 229/597 [05:24<11:36,  1.89s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  39%|███▊      | 230/597 [05:24<09:20,  1.53s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  39%|███▊      | 231/597 [05:26<08:50,  1.45s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  39%|███▉      | 232/597 [05:27<08:33,  1.41s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  39%|███▉      | 233/597 [05:28<07:33,  1.25s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  39%|███▉      | 234/597 [05:29<06:51,  1.13s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  39%|███▉      | 235/597 [05:29<05:50,  1.03it/s]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  40%|███▉      | 236/597 [05:31<07:15,  1.21s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  40%|███▉      | 237/597 [05:32<06:21,  1.06s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  40%|███▉      | 238/597 [05:33<06:41,  1.12s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  40%|████      | 239/597 [05:36<09:18,  1.56s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  40%|████      | 240/597 [05:37<08:19,  1.40s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  40%|████      | 241/597 [05:37<07:16,  1.23s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  41%|████      | 242/597 [05:39<07:08,  1.21s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  41%|████      | 243/597 [05:39<06:19,  1.07s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  41%|████      | 244/597 [05:40<06:07,  1.04s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  41%|████      | 245/597 [05:42<06:24,  1.09s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  41%|████      | 246/597 [05:44<09:00,  1.54s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  41%|████▏     | 247/597 [05:45<08:36,  1.48s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  42%|████▏     | 248/597 [05:46<07:27,  1.28s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  42%|████▏     | 249/597 [05:47<06:32,  1.13s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  42%|████▏     | 250/597 [05:50<10:31,  1.82s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  42%|████▏     | 251/597 [05:52<09:39,  1.67s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  42%|████▏     | 252/597 [05:54<09:46,  1.70s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  42%|████▏     | 253/597 [05:54<08:20,  1.46s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  43%|████▎     | 254/597 [05:55<07:36,  1.33s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  43%|████▎     | 255/597 [05:58<09:47,  1.72s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  43%|████▎     | 256/597 [05:59<08:04,  1.42s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  43%|████▎     | 257/597 [06:01<08:36,  1.52s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  43%|████▎     | 258/597 [06:02<08:08,  1.44s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  43%|████▎     | 259/597 [06:03<07:38,  1.36s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  44%|████▎     | 260/597 [06:06<10:07,  1.80s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  44%|████▎     | 261/597 [06:07<08:46,  1.57s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  44%|████▍     | 262/597 [06:08<08:10,  1.46s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  44%|████▍     | 263/597 [06:09<07:08,  1.28s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  44%|████▍     | 264/597 [06:10<06:11,  1.11s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  44%|████▍     | 265/597 [06:11<06:00,  1.09s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  45%|████▍     | 266/597 [06:12<06:11,  1.12s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  45%|████▍     | 267/597 [06:13<05:56,  1.08s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  45%|████▍     | 268/597 [06:14<05:36,  1.02s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  45%|████▌     | 269/597 [06:18<10:15,  1.88s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  45%|████▌     | 270/597 [06:19<08:40,  1.59s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  45%|████▌     | 271/597 [06:21<09:12,  1.70s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  46%|████▌     | 272/597 [06:21<07:50,  1.45s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  46%|████▌     | 273/597 [06:22<06:39,  1.23s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  46%|████▌     | 274/597 [06:23<06:36,  1.23s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  46%|████▌     | 275/597 [06:24<06:29,  1.21s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  46%|████▌     | 276/597 [06:27<07:48,  1.46s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  46%|████▋     | 277/597 [06:27<06:47,  1.27s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  47%|████▋     | 278/597 [06:28<06:07,  1.15s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  47%|████▋     | 279/597 [06:29<06:07,  1.16s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  47%|████▋     | 280/597 [06:31<06:30,  1.23s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  47%|████▋     | 281/597 [06:34<09:18,  1.77s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  47%|████▋     | 282/597 [06:35<08:34,  1.63s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  47%|████▋     | 283/597 [06:36<08:01,  1.53s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  48%|████▊     | 284/597 [06:37<06:48,  1.30s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  48%|████▊     | 285/597 [06:39<07:02,  1.35s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  48%|████▊     | 286/597 [06:41<07:51,  1.52s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  48%|████▊     | 287/597 [06:42<07:43,  1.50s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  48%|████▊     | 288/597 [06:44<08:31,  1.65s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  48%|████▊     | 289/597 [06:45<07:08,  1.39s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  49%|████▊     | 290/597 [06:47<08:04,  1.58s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  49%|████▊     | 291/597 [06:48<08:01,  1.57s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  49%|████▉     | 292/597 [06:51<09:22,  1.84s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  49%|████▉     | 293/597 [06:53<09:17,  1.83s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  49%|████▉     | 294/597 [06:54<08:06,  1.60s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  49%|████▉     | 295/597 [06:55<07:30,  1.49s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  50%|████▉     | 296/597 [06:56<06:28,  1.29s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  50%|████▉     | 297/597 [06:57<06:29,  1.30s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  50%|████▉     | 298/597 [06:58<06:08,  1.23s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  50%|█████     | 299/597 [07:00<06:15,  1.26s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  50%|█████     | 300/597 [07:01<06:11,  1.25s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  50%|█████     | 301/597 [07:02<06:41,  1.36s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  51%|█████     | 302/597 [07:03<06:02,  1.23s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  51%|█████     | 303/597 [07:05<06:09,  1.26s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  51%|█████     | 304/597 [07:06<05:55,  1.21s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  51%|█████     | 305/597 [07:07<06:26,  1.32s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  51%|█████▏    | 306/597 [07:09<07:21,  1.52s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  51%|█████▏    | 307/597 [07:11<07:13,  1.50s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  52%|█████▏    | 308/597 [07:12<06:35,  1.37s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  52%|█████▏    | 309/597 [07:13<06:23,  1.33s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  52%|█████▏    | 310/597 [07:15<07:34,  1.58s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  52%|█████▏    | 311/597 [07:17<07:35,  1.59s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  52%|█████▏    | 312/597 [07:19<08:01,  1.69s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  52%|█████▏    | 313/597 [07:21<08:15,  1.74s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  53%|█████▎    | 314/597 [07:22<08:03,  1.71s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  53%|█████▎    | 315/597 [07:24<07:23,  1.57s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  53%|█████▎    | 316/597 [07:25<07:12,  1.54s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  53%|█████▎    | 317/597 [07:26<06:56,  1.49s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  53%|█████▎    | 318/597 [07:28<06:40,  1.44s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  53%|█████▎    | 319/597 [07:29<05:53,  1.27s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  54%|█████▎    | 320/597 [07:29<05:02,  1.09s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  54%|█████▍    | 321/597 [07:31<06:05,  1.33s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  54%|█████▍    | 322/597 [07:33<07:06,  1.55s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  54%|█████▍    | 323/597 [07:36<08:17,  1.81s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  54%|█████▍    | 324/597 [07:38<09:21,  2.06s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  54%|█████▍    | 325/597 [07:39<07:47,  1.72s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  55%|█████▍    | 326/597 [07:40<06:32,  1.45s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  55%|█████▍    | 327/597 [07:41<05:40,  1.26s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  55%|█████▍    | 328/597 [07:42<05:35,  1.25s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  55%|█████▌    | 329/597 [07:45<07:22,  1.65s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  55%|█████▌    | 330/597 [07:46<06:59,  1.57s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  55%|█████▌    | 331/597 [07:47<06:09,  1.39s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  56%|█████▌    | 332/597 [07:48<05:20,  1.21s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  56%|█████▌    | 333/597 [07:49<05:28,  1.24s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  56%|█████▌    | 334/597 [07:50<05:29,  1.25s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  56%|█████▌    | 335/597 [07:51<04:50,  1.11s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  56%|█████▋    | 336/597 [07:52<04:07,  1.05it/s]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  56%|█████▋    | 337/597 [07:53<04:00,  1.08it/s]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  57%|█████▋    | 338/597 [07:55<05:49,  1.35s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  57%|█████▋    | 339/597 [07:56<05:30,  1.28s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  57%|█████▋    | 340/597 [07:57<05:16,  1.23s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  57%|█████▋    | 341/597 [07:58<04:59,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  57%|█████▋    | 342/597 [07:59<04:46,  1.12s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  57%|█████▋    | 343/597 [08:01<05:54,  1.40s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  58%|█████▊    | 344/597 [08:03<05:48,  1.38s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  58%|█████▊    | 345/597 [08:04<05:54,  1.41s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  58%|█████▊    | 346/597 [08:05<05:42,  1.37s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  58%|█████▊    | 347/597 [08:08<07:34,  1.82s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  58%|█████▊    | 348/597 [08:09<06:38,  1.60s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  58%|█████▊    | 349/597 [08:11<06:16,  1.52s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  59%|█████▊    | 350/597 [08:13<07:47,  1.89s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  59%|█████▉    | 351/597 [08:14<06:49,  1.67s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  59%|█████▉    | 352/597 [08:16<06:09,  1.51s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  59%|█████▉    | 353/597 [08:17<06:06,  1.50s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  59%|█████▉    | 354/597 [08:19<06:51,  1.69s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  59%|█████▉    | 355/597 [08:20<06:09,  1.53s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  60%|█████▉    | 356/597 [08:22<06:17,  1.57s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  60%|█████▉    | 357/597 [08:23<05:32,  1.39s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  60%|█████▉    | 358/597 [08:24<05:05,  1.28s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  60%|██████    | 359/597 [08:25<05:03,  1.27s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  60%|██████    | 360/597 [08:27<05:12,  1.32s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  60%|██████    | 361/597 [08:29<06:09,  1.57s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  61%|██████    | 362/597 [08:29<04:44,  1.21s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  61%|██████    | 363/597 [08:32<06:17,  1.61s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  61%|██████    | 364/597 [08:33<06:02,  1.56s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  61%|██████    | 365/597 [08:35<06:14,  1.61s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  61%|██████▏   | 366/597 [08:37<06:28,  1.68s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  61%|██████▏   | 367/597 [08:38<06:26,  1.68s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  62%|██████▏   | 368/597 [08:39<05:23,  1.41s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  62%|██████▏   | 369/597 [08:40<04:55,  1.30s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  62%|██████▏   | 370/597 [08:43<06:14,  1.65s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  62%|██████▏   | 371/597 [08:45<06:21,  1.69s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  62%|██████▏   | 372/597 [08:45<05:28,  1.46s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  62%|██████▏   | 373/597 [08:47<05:18,  1.42s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  63%|██████▎   | 374/597 [08:49<06:20,  1.71s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  63%|██████▎   | 375/597 [08:50<05:50,  1.58s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  63%|██████▎   | 376/597 [08:52<05:21,  1.46s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  63%|██████▎   | 377/597 [08:53<05:22,  1.46s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  63%|██████▎   | 378/597 [08:55<05:41,  1.56s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  63%|██████▎   | 379/597 [08:56<05:35,  1.54s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  64%|██████▎   | 380/597 [08:59<07:02,  1.94s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  64%|██████▍   | 381/597 [09:03<08:56,  2.48s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  64%|██████▍   | 382/597 [09:05<08:18,  2.32s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  64%|██████▍   | 383/597 [09:06<07:05,  1.99s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  64%|██████▍   | 384/597 [09:07<06:11,  1.74s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  64%|██████▍   | 385/597 [09:10<07:30,  2.12s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  65%|██████▍   | 386/597 [09:11<06:26,  1.83s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  65%|██████▍   | 387/597 [09:13<05:37,  1.61s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  65%|██████▍   | 388/597 [09:14<05:02,  1.45s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  65%|██████▌   | 389/597 [09:15<04:41,  1.35s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  65%|██████▌   | 390/597 [09:16<04:34,  1.32s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  65%|██████▌   | 391/597 [09:17<04:34,  1.33s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  66%|██████▌   | 392/597 [09:19<04:29,  1.32s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  66%|██████▌   | 393/597 [09:21<05:30,  1.62s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  66%|██████▌   | 394/597 [09:22<04:59,  1.48s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  66%|██████▌   | 395/597 [09:25<06:30,  1.93s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  66%|██████▋   | 396/597 [09:26<05:49,  1.74s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  66%|██████▋   | 397/597 [09:29<06:09,  1.85s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  67%|██████▋   | 398/597 [09:29<05:12,  1.57s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  67%|██████▋   | 399/597 [09:30<04:20,  1.32s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  67%|██████▋   | 400/597 [09:32<04:57,  1.51s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  67%|██████▋   | 401/597 [09:33<04:24,  1.35s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  67%|██████▋   | 402/597 [09:35<04:39,  1.43s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  68%|██████▊   | 403/597 [09:37<04:58,  1.54s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  68%|██████▊   | 404/597 [09:38<04:50,  1.50s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  68%|██████▊   | 405/597 [09:39<04:17,  1.34s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  68%|██████▊   | 406/597 [09:40<04:06,  1.29s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  68%|██████▊   | 407/597 [09:41<04:01,  1.27s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  68%|██████▊   | 408/597 [09:43<04:11,  1.33s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  69%|██████▊   | 409/597 [09:44<04:08,  1.32s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  69%|██████▊   | 410/597 [09:46<04:27,  1.43s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  69%|██████▉   | 411/597 [09:47<04:09,  1.34s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  69%|██████▉   | 412/597 [09:50<05:26,  1.77s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  69%|██████▉   | 413/597 [09:51<04:46,  1.56s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  69%|██████▉   | 414/597 [09:52<04:38,  1.52s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  70%|██████▉   | 415/597 [09:53<04:26,  1.46s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  70%|██████▉   | 416/597 [09:54<03:54,  1.30s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  70%|██████▉   | 417/597 [09:55<03:42,  1.23s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  70%|███████   | 418/597 [09:58<05:07,  1.72s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  70%|███████   | 419/597 [09:59<04:33,  1.54s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  70%|███████   | 420/597 [10:01<04:12,  1.43s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  71%|███████   | 421/597 [10:02<04:02,  1.38s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  71%|███████   | 422/597 [10:03<03:44,  1.28s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  71%|███████   | 423/597 [10:04<03:42,  1.28s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  71%|███████   | 424/597 [10:05<03:22,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  71%|███████   | 425/597 [10:06<03:06,  1.08s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  71%|███████▏  | 426/597 [10:08<03:28,  1.22s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  72%|███████▏  | 427/597 [10:09<03:35,  1.27s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  72%|███████▏  | 428/597 [10:10<03:13,  1.15s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  72%|███████▏  | 429/597 [10:12<04:04,  1.45s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  72%|███████▏  | 430/597 [10:13<04:06,  1.47s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  72%|███████▏  | 431/597 [10:16<04:34,  1.66s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  72%|███████▏  | 432/597 [10:17<04:07,  1.50s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  73%|███████▎  | 433/597 [10:18<03:54,  1.43s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  73%|███████▎  | 434/597 [10:19<03:51,  1.42s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  73%|███████▎  | 435/597 [10:21<03:40,  1.36s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  73%|███████▎  | 436/597 [10:22<04:04,  1.52s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  73%|███████▎  | 437/597 [10:24<03:46,  1.42s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  73%|███████▎  | 438/597 [10:25<03:54,  1.47s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  74%|███████▎  | 439/597 [10:26<03:35,  1.37s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  74%|███████▎  | 440/597 [10:27<03:13,  1.23s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  74%|███████▍  | 441/597 [10:28<02:52,  1.10s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  74%|███████▍  | 442/597 [10:32<04:39,  1.80s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  74%|███████▍  | 443/597 [10:32<03:57,  1.54s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  74%|███████▍  | 444/597 [10:34<03:39,  1.43s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  75%|███████▍  | 445/597 [10:34<03:01,  1.19s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  75%|███████▍  | 446/597 [10:36<03:08,  1.25s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  75%|███████▍  | 447/597 [10:37<03:23,  1.36s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  75%|███████▌  | 448/597 [10:39<03:31,  1.42s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  75%|███████▌  | 449/597 [10:41<04:17,  1.74s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  75%|███████▌  | 450/597 [10:42<03:44,  1.52s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  76%|███████▌  | 451/597 [10:44<04:06,  1.69s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  76%|███████▌  | 452/597 [10:46<03:42,  1.53s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  76%|███████▌  | 453/597 [10:47<03:27,  1.44s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  76%|███████▌  | 454/597 [10:49<03:47,  1.59s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  76%|███████▌  | 455/597 [10:50<03:51,  1.63s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  76%|███████▋  | 456/597 [10:53<04:07,  1.75s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  77%|███████▋  | 457/597 [10:54<03:50,  1.64s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  77%|███████▋  | 458/597 [10:55<03:19,  1.43s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  77%|███████▋  | 459/597 [10:57<03:39,  1.59s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  77%|███████▋  | 460/597 [10:58<03:39,  1.60s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  77%|███████▋  | 461/597 [11:00<03:51,  1.70s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  77%|███████▋  | 462/597 [11:01<03:24,  1.52s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  78%|███████▊  | 463/597 [11:03<03:35,  1.61s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  78%|███████▊  | 464/597 [11:05<03:24,  1.54s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  78%|███████▊  | 465/597 [11:08<04:46,  2.17s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  78%|███████▊  | 466/597 [11:09<04:05,  1.87s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  78%|███████▊  | 467/597 [11:12<04:09,  1.92s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  78%|███████▊  | 468/597 [11:13<03:37,  1.68s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  79%|███████▊  | 469/597 [11:14<03:10,  1.48s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  79%|███████▊  | 470/597 [11:15<03:02,  1.44s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  79%|███████▉  | 471/597 [11:16<02:44,  1.30s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  79%|███████▉  | 472/597 [11:18<02:57,  1.42s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  79%|███████▉  | 473/597 [11:19<02:37,  1.27s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  79%|███████▉  | 474/597 [11:21<03:17,  1.60s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  80%|███████▉  | 475/597 [11:22<03:05,  1.52s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  80%|███████▉  | 476/597 [11:23<02:45,  1.37s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  80%|███████▉  | 477/597 [11:25<02:53,  1.45s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  80%|████████  | 478/597 [11:27<03:16,  1.65s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  80%|████████  | 479/597 [11:28<02:49,  1.43s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  80%|████████  | 480/597 [11:29<02:31,  1.30s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  81%|████████  | 481/597 [11:30<02:21,  1.22s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  81%|████████  | 482/597 [11:31<02:08,  1.12s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  81%|████████  | 483/597 [11:32<02:01,  1.06s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  81%|████████  | 484/597 [11:33<01:59,  1.05s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  81%|████████  | 485/597 [11:34<02:17,  1.23s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  81%|████████▏ | 486/597 [11:36<02:16,  1.23s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  82%|████████▏ | 487/597 [11:37<02:15,  1.23s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  82%|████████▏ | 488/597 [11:39<02:36,  1.44s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  82%|████████▏ | 489/597 [11:40<02:39,  1.48s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  82%|████████▏ | 490/597 [11:43<03:01,  1.70s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  82%|████████▏ | 491/597 [11:44<02:36,  1.47s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  82%|████████▏ | 492/597 [11:45<02:32,  1.45s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  83%|████████▎ | 493/597 [11:47<02:33,  1.47s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  83%|████████▎ | 494/597 [11:48<02:29,  1.45s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  83%|████████▎ | 495/597 [11:50<02:43,  1.61s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  83%|████████▎ | 496/597 [11:51<02:26,  1.45s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  83%|████████▎ | 497/597 [11:53<02:32,  1.52s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  83%|████████▎ | 498/597 [11:54<02:25,  1.47s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  84%|████████▎ | 499/597 [11:55<02:22,  1.46s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  84%|████████▍ | 500/597 [11:56<02:07,  1.31s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  84%|████████▍ | 501/597 [11:58<01:59,  1.24s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  84%|████████▍ | 502/597 [11:59<02:04,  1.31s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  84%|████████▍ | 503/597 [12:01<02:15,  1.44s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  84%|████████▍ | 504/597 [12:02<02:09,  1.39s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  85%|████████▍ | 505/597 [12:04<02:35,  1.69s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  85%|████████▍ | 506/597 [12:06<02:30,  1.66s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  85%|████████▍ | 507/597 [12:07<02:07,  1.42s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  85%|████████▌ | 508/597 [12:09<02:18,  1.55s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  85%|████████▌ | 509/597 [12:10<02:11,  1.50s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  85%|████████▌ | 510/597 [12:11<01:57,  1.35s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  86%|████████▌ | 511/597 [12:12<01:45,  1.22s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  86%|████████▌ | 512/597 [12:13<01:42,  1.21s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  86%|████████▌ | 513/597 [12:14<01:44,  1.24s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  86%|████████▌ | 514/597 [12:16<01:39,  1.20s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  86%|████████▋ | 515/597 [12:17<01:39,  1.21s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  86%|████████▋ | 516/597 [12:18<01:27,  1.08s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  87%|████████▋ | 517/597 [12:19<01:24,  1.06s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  87%|████████▋ | 518/597 [12:21<02:01,  1.54s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  87%|████████▋ | 519/597 [12:23<01:54,  1.47s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  87%|████████▋ | 520/597 [12:24<01:48,  1.41s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  87%|████████▋ | 521/597 [12:25<01:46,  1.40s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  87%|████████▋ | 522/597 [12:27<01:48,  1.44s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  88%|████████▊ | 523/597 [12:28<01:41,  1.38s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  88%|████████▊ | 524/597 [12:29<01:36,  1.32s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  88%|████████▊ | 525/597 [12:31<01:35,  1.33s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  88%|████████▊ | 526/597 [12:32<01:31,  1.28s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  88%|████████▊ | 527/597 [12:33<01:25,  1.22s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  88%|████████▊ | 528/597 [12:34<01:33,  1.36s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  89%|████████▊ | 529/597 [12:36<01:40,  1.48s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  89%|████████▉ | 530/597 [12:38<01:44,  1.57s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  89%|████████▉ | 531/597 [12:39<01:38,  1.49s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  89%|████████▉ | 532/597 [12:41<01:34,  1.46s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  89%|████████▉ | 533/597 [12:42<01:32,  1.45s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  89%|████████▉ | 534/597 [12:43<01:26,  1.38s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  90%|████████▉ | 535/597 [12:44<01:12,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  90%|████████▉ | 536/597 [12:45<01:07,  1.11s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  90%|████████▉ | 537/597 [12:47<01:31,  1.52s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  90%|█████████ | 538/597 [12:49<01:23,  1.42s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  90%|█████████ | 539/597 [12:50<01:16,  1.33s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  90%|█████████ | 540/597 [12:51<01:09,  1.22s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  91%|█████████ | 541/597 [12:52<01:06,  1.18s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  91%|█████████ | 542/597 [12:53<01:12,  1.31s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  91%|█████████ | 543/597 [12:55<01:08,  1.27s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  91%|█████████ | 544/597 [12:56<01:12,  1.38s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  91%|█████████▏| 545/597 [12:57<01:06,  1.29s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  91%|█████████▏| 546/597 [12:59<01:07,  1.33s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  92%|█████████▏| 547/597 [13:00<01:07,  1.35s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  92%|█████████▏| 548/597 [13:01<00:58,  1.20s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  92%|█████████▏| 549/597 [13:03<01:11,  1.49s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  92%|█████████▏| 550/597 [13:04<01:07,  1.43s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  92%|█████████▏| 551/597 [13:07<01:21,  1.77s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  92%|█████████▏| 552/597 [13:09<01:19,  1.76s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  93%|█████████▎| 553/597 [13:11<01:20,  1.82s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  93%|█████████▎| 554/597 [13:12<01:11,  1.67s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  93%|█████████▎| 555/597 [13:13<01:01,  1.46s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  93%|█████████▎| 556/597 [13:15<01:01,  1.49s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  93%|█████████▎| 557/597 [13:17<01:09,  1.73s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  93%|█████████▎| 558/597 [13:18<00:56,  1.44s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  94%|█████████▎| 559/597 [13:19<00:50,  1.34s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  94%|█████████▍| 560/597 [13:21<00:59,  1.60s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  94%|█████████▍| 561/597 [13:22<00:52,  1.46s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  94%|█████████▍| 562/597 [13:23<00:44,  1.27s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  94%|█████████▍| 563/597 [13:24<00:39,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  94%|█████████▍| 564/597 [13:25<00:43,  1.31s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  95%|█████████▍| 565/597 [13:26<00:37,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  95%|█████████▍| 566/597 [13:27<00:31,  1.02s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  95%|█████████▍| 567/597 [13:28<00:28,  1.07it/s]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  95%|█████████▌| 568/597 [13:30<00:37,  1.29s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  95%|█████████▌| 569/597 [13:31<00:37,  1.33s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  95%|█████████▌| 570/597 [13:33<00:36,  1.35s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  96%|█████████▌| 571/597 [13:34<00:33,  1.30s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  96%|█████████▌| 572/597 [13:35<00:34,  1.36s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  96%|█████████▌| 573/597 [13:37<00:35,  1.48s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  96%|█████████▌| 574/597 [13:38<00:28,  1.24s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  96%|█████████▋| 575/597 [13:39<00:25,  1.18s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  96%|█████████▋| 576/597 [13:41<00:32,  1.53s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  97%|█████████▋| 577/597 [13:42<00:28,  1.45s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  97%|█████████▋| 578/597 [13:45<00:31,  1.65s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  97%|█████████▋| 579/597 [13:46<00:26,  1.47s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  97%|█████████▋| 580/597 [13:48<00:27,  1.60s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  97%|█████████▋| 581/597 [13:49<00:22,  1.43s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  97%|█████████▋| 582/597 [13:51<00:26,  1.79s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  98%|█████████▊| 583/597 [13:53<00:26,  1.90s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  98%|█████████▊| 584/597 [13:54<00:21,  1.62s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  98%|█████████▊| 585/597 [13:55<00:17,  1.46s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  98%|█████████▊| 586/597 [13:57<00:18,  1.64s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  98%|█████████▊| 587/597 [13:58<00:13,  1.36s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  98%|█████████▊| 588/597 [13:59<00:11,  1.29s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  99%|█████████▊| 589/597 [14:00<00:09,  1.23s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  99%|█████████▉| 590/597 [14:02<00:08,  1.22s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  99%|█████████▉| 591/597 [14:02<00:06,  1.10s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  99%|█████████▉| 592/597 [14:04<00:05,  1.16s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  99%|█████████▉| 593/597 [14:05<00:04,  1.14s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses:  99%|█████████▉| 594/597 [14:07<00:04,  1.59s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses: 100%|█████████▉| 595/597 [14:08<00:02,  1.41s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses: 100%|█████████▉| 596/597 [14:10<00:01,  1.37s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Generating responses: 100%|██████████| 597/597 [14:12<00:00,  1.43s/it]\n",
      "c:\\Users\\BMSCE CSE\\Desktop\\Instruct\\myenv\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "c:\\Users\\BMSCE CSE\\Desktop\\Instruct\\myenv\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "c:\\Users\\BMSCE CSE\\Desktop\\Instruct\\myenv\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Scores: [1.3657076305145602e-231, 1.750652446866496e-78, 2.450587819129029e-78, 1.342376123197744e-232, 8.775719106254692e-232, 5.689469354416895e-155, 0.170595737016168, 0.23934337045316806, 1.0749662678465409e-231, 0.2574866101628968, 3.6449283809869995e-155, 2.437021513410318e-78, 8.166726842395623e-232, 4.177908019702813e-155, 1.1302704678971024e-231, 2.972520735696054e-78, 1.1640469867513693e-231, 1.948816807483985e-78, 1.268852357850863e-231, 5.936729290317312e-155, 1.0004457072019472e-231, 1.6867123346060813e-155, 4.16375600153148e-155, 6.6379988158219165e-155, 4.90155371720033e-155, 1.1200407237786664e-231, 5.110386103471128e-155, 6.08970970641905e-155, 1.0518351895246305e-231, 1.4488496539373276e-231, 1.9192217019020802e-78, 7.677476567795425e-155, 1.0244914152188952e-231, 0, 2.0581673600881486e-78, 0.08083346489298068, 1.2558634180836711e-231, 0.1585266710543935, 3.899047496415677e-155, 0.14183098603897615, 4.992080652587298e-155, 4.500105988612376e-155, 9.594503055152632e-232, 0.13092816081223754, 2.881736906133446e-78, 5.909363526536785e-155, 7.346153272462044e-155, 0.2626909894424158, 0, 8.833570818496756e-232, 6.5900678916581e-155, 5.244712163973215e-232, 4.2338646970116024e-78, 7.407523983575941e-79, 6.13274920178966e-155, 1.859764150340038e-78, 5.180717820722376e-155, 2.3158251789104014e-155, 6.556394747231264e-155, 0.1487964117124549, 1.2508498911928379e-231, 4.262352868760118e-155, 1.328488253493217e-231, 1.1200407237786664e-231, 6.164444051541174e-155, 6.373704167435469e-155, 7.680275217209526e-232, 1.1454162233972301e-231, 1.7281650139551376e-78, 3.1953179281650384e-155, 0.23287896954139947, 5.052063697963686e-155, 2.2924520635667424e-78, 1.3091834502273125e-231, 0.4111336169005197, 6.1773539527177615e-155, 2.6312854176309787e-155, 9.594503055152632e-232, 1.2874956263263696e-231, 0, 1.2882297539194154e-231, 0.10509349950390642, 4.401623672257616e-155, 6.1683028603484695e-155, 5.554837769749797e-155, 5.233427736988301e-155, 0.13565829306292676, 5.1945147945938015e-155, 4.905470711005226e-155, 0.14216645907653844, 1.0832677820940877e-231, 0.170595737016168, 0, 1.7636164817806938e-78, 9.788429383461836e-232, 1.0244914152188952e-231, 0, 4.282289549833951e-232, 2.506862211133605e-78, 1.268852357850863e-231, 2.0224285973329676e-78, 1.6964005159142612e-78, 1.0976852611324293e-231, 1.0832677820940877e-231, 1.0244914152188952e-231, 4.3638512969692e-232, 2.114819568967012e-155, 3.40575823513205e-155, 2.894557470629093e-155, 0, 3.0613502729846216e-78, 6.4093713431280374e-155, 2.4995820309141096e-78, 2.719841156936326e-78, 1.268852357850863e-231, 1.0028226953690653e-231, 1.2356563982938026e-231, 0.12624426670185632, 4.693773976175812e-155, 5.157006819435075e-155, 1.4147351699132998e-231, 1.1388294875424009e-231, 0, 5.431635669940757e-155, 4.83404070852666e-155, 1.909398934417538e-155, 1.0832677820940877e-231, 4.805276545626372e-155, 1.9338448203264796e-78, 0.1049439266125276, 8.317436423186365e-232, 2.5751259695884524e-78, 8.269750081803823e-232, 1.0704886386843988e-78, 4.189519554666584e-232, 7.02464720468843e-232, 1.012071042130996e-231, 6.538936929519805e-232, 1.1193096620723278e-231, 6.836355173434557e-232, 2.3160821182153625e-232, 0.16192855291240682, 1.331960397810445e-231, 6.424810961602989e-155, 0.18207052811092134, 4.039680371135726e-155, 5.0437883815744365e-155, 3.165087820262515e-155, 2.9789301474407864e-155, 4.174080136286982e-155, 0, 3.648956622645728e-232, 7.518122319169092e-232, 8.844844403089352e-232, 2.0960793397093405e-78, 2.5337984351753685e-155, 4.717068855239749e-155, 1.1808001696991507e-231, 2.0328072460442305e-78, 7.389560029878221e-232, 5.5818694086831284e-155, 0.16811508624309293, 6.0355388539751725e-155, 2.7354554660167096e-155, 9.788429383461836e-232, 2.3066868057461354e-78, 2.9559907859786378e-78, 1.8742342044920768e-155, 4.452250377698693e-155, 5.667113443314699e-155, 2.5460591799018596e-78, 9.314777610904016e-232, 3.845705588920993e-155, 3.910521471383976e-155, 0, 1.0024921848014553e-231, 2.7920648233755865e-232, 2.281256075253767e-78, 1.1023928157275093e-231, 0, 9.065607048138757e-232, 4.092374282972667e-155, 2.345608475430466e-78, 6.058331755668576e-232, 1.8765280777992895e-155, 1.1008876702055895e-231, 1.1640469867513693e-231, 2.584433612192727e-78, 4.995008821347951e-233, 1.1896457329133973e-231, 6.895396983435736e-232, 1.8347562739787016e-78, 8.972141065609098e-232, 1.3091834502273125e-231, 0, 2.0038552016204957e-78, 3.231681603406294e-78, 6.5900678916581e-155, 1.1896457329133973e-231, 5.969061643530969e-155, 1.0244914152188952e-231, 3.829599714007002e-155, 4.282289549833951e-232, 3.352743298097415e-155, 4.336508547901975e-155, 0, 0, 0.1763976985646294, 0.11686071085472406, 9.834445636575866e-232, 7.313559129677039e-232, 2.626847702203086e-78, 3.410235676232928e-232, 2.9240088784488366e-78, 0.07253906792122727, 1.3017285428067626e-233, 0.38890556115271097, 4.481994719908145e-232, 5.71648488068152e-232, 4.7030228367359436e-155, 0.360056585428503, 2.272540691926962e-78, 2.415892461334762e-78, 8.285726588482745e-232, 0.08635800047213178, 1.0016022933125248e-231, 4.3346579089507125e-155, 1.325250056439003e-231, 0.06856374867510526, 4.78084292331513e-233, 1.447860902517854e-78, 5.859491021246916e-155, 9.788429383461836e-232, 6.836355173434557e-232, 1.7984638527808529e-78, 1.3531653690559654e-231, 3.6348497300557706e-78, 1.1337861261109773e-231, 1.0423153399406431e-231, 5.18112814477353e-232, 6.686350417856737e-232, 6.747523154432037e-155, 4.59101586365559e-155, 2.6165216836194074e-155, 0.2061477352156375, 3.6951568549677833e-155, 1.3094850321578048e-231, 0, 2.3447547273082534e-155, 4.978826010263473e-155, 1.0377133938315695e-231, 6.887578243315168e-155, 1.2183324802375697e-231, 9.788429383461836e-232, 0.28592291256793106, 9.853445011990208e-232, 9.50440384721771e-232, 0, 4.864499739012968e-232, 0.38055022071259326, 3.4558505924538693e-78, 1.0398482420380189e-78, 5.092529201164552e-232, 0.7259795291154771, 3.855182486401551e-155, 1.771334343765161e-155, 7.142069717037071e-155, 1.5470341759091752e-155, 9.788429383461836e-232, 4.27242167093289e-155, 5.4931618878649074e-155, 1.2183324802375697e-231, 4.739132419722992e-232, 6.6173482962526626e-155, 5.667113443314699e-155, 1.1102577717991281e-231, 6.836355173434557e-232, 2.405540463487053e-232, 0.29470691492333945, 0.1196655750514248, 4.393655368849994e-155, 4.905470711005226e-155, 2.787124325581721e-78, 1.1321305733852208e-155, 1.2035620841904511e-231, 1.1008876702055895e-231, 2.0454628061435778e-232, 0, 5.7592918561109494e-155, 1.1008876702055895e-231, 9.893133360884868e-232, 0.19114167548796013, 4.905470711005226e-155, 5.110503616121594e-155, 2.8638898911433973e-155, 6.805395922591311e-155, 3.672581505594886e-155, 2.4301573623870167e-155, 9.485497713164327e-232, 6.061838450024688e-155, 9.893133360884868e-232, 9.594503055152632e-232, 1.4386816597928502e-231, 2.415539400799213e-78, 1.0244914152188952e-231, 5.681968389717772e-155, 3.963794078327478e-155, 2.2652057167541336e-78, 7.813508425061864e-232, 4.298631483271809e-155, 2.1797081356031365e-78, 9.257324954728539e-232, 4.884188340600192e-155, 9.689041594391036e-232, 4.38810481436839e-155, 1.1927381921713823e-231, 0.2886262071503404, 4.693773976175812e-155, 1.4256605770826504e-231, 0, 2.2080919053537955e-78, 2.1797081356031365e-78, 4.583254795053136e-155, 0, 3.047091569143471e-78, 0, 3.648888025177688e-232, 1.2247502623754442e-231, 1.3509119634545632e-231, 9.4553458751165e-232, 3.3898826267936662e-155, 3.5879471085711124e-78, 2.9857029691673486e-78, 5.163239668067908e-155, 0, 4.740699245976165e-79, 5.714667529410374e-155, 3.812533760698053e-155, 6.53175655370481e-155, 6.820283718722585e-232, 1.2627076138080564e-231, 5.5986234319889216e-232, 0, 1.331960397810445e-231, 5.035580399326033e-155, 1.3483065280626046e-231, 9.039352811507815e-232, 8.769105200491155e-232, 0.43319877589068106, 1.0003688322288243e-231, 2.512875519123926e-78, 4.327158966058527e-155, 1.006020563316025e-231, 4.927224994977926e-155, 3.4568450486098256e-155, 1.2183324802375697e-231, 3.5318487635731415e-78, 7.57965434483665e-155, 4.90389738784068e-155, 1.2882297539194154e-231, 4.336508547901975e-155, 0, 7.114448003820407e-232, 6.202551001158085e-155, 0.12794810631258222, 3.5131878770278888e-155, 4.623615267576204e-155, 1.65807896920556e-78, 6.356121367760845e-155, 5.808311525655002e-155, 4.023062361861117e-155, 5.116994679231889e-232, 2.7662422404204523e-78, 2.23451899928512e-78, 1.2882297539194154e-231, 4.818674597335635e-155, 5.157006819435075e-155, 4.452250377698693e-155, 2.0795115571777008e-78, 0.14326891642560133, 0.07296554923386446, 0, 1.4226350938547638e-78, 7.060301868108111e-232, 0, 5.058927350602078e-232, 4.84335144972907e-232, 4.417454612514246e-155, 8.261779275311711e-232, 7.677476567795425e-155, 1.9090934557449592e-78, 8.844844403089352e-232, 1.0244914152188952e-231, 1.0603559004092113e-231, 4.801280454758726e-232, 1.1337861261109773e-231, 1.1722739790570059e-231, 1.861075366666417e-232, 1.0518351895246305e-231, 3.4437891216575777e-155, 0.22718709780542318, 0.195647514979229, 1.184298875206812e-231, 5.9883480390466785e-155, 9.788429383461836e-232, 1.9282978281717605e-78, 4.7723184710901465e-155, 1.2356563982938026e-231, 2.8826653224513366e-78, 7.27128890699765e-232, 4.213850193469532e-155, 8.844844403089352e-232, 8.557263845682554e-232, 4.13918559701879e-155, 5.157006819435075e-155, 9.918706012922318e-232, 0.10315156032159925, 9.257324954728539e-232, 9.668151865621759e-232, 0.2015767529554973, 9.320959610444274e-232, 3.2420559461356048e-155, 7.724841230759937e-232, 5.481805263404418e-232, 1.0003688322288243e-231, 4.446380555605557e-155, 5.635809992474887e-232, 0.21200626759025185, 1.0592691162555667e-231, 3.3673841946428456e-155, 1.2183324802375697e-231, 1.805707305669354e-78, 8.972141065609098e-232, 1.268852357850863e-231, 7.95096645136912e-232, 4.336508547901975e-155, 5.333156893552256e-155, 9.594503055152632e-232, 2.570061783884003e-78, 7.855726910762709e-155, 0, 1.7745179415358185e-78, 9.282687447692475e-232, 4.9948762082130405e-155, 4.679577191845681e-155, 4.443183005753097e-155, 8.147480343967206e-232, 1.0649934005834102e-231, 3.4626327315717625e-155, 1.1200407237786664e-231, 0, 0.09331639814928933, 4.693773976175812e-155, 5.22029481607517e-155, 5.795860625496088e-155, 5.969061643530969e-155, 1.3109733799909448e-231, 4.926859605505761e-155, 9.689041594391036e-232, 1.2064448882743937e-231, 1.331960397810445e-231, 1.1988328686372911e-231, 4.619180139913729e-155, 1.1764984582435574e-231, 1.1102577717991281e-231, 2.52727686648381e-78, 5.667113443314699e-155, 3.4835506189939298e-155, 1.2114962633177682e-231, 1.7766352554232615e-232, 1.5830994218055544e-155, 2.5985740997266745e-155, 0.4703709593866897, 1.1200407237786664e-231, 4.215713836781796e-155, 0, 1.2183324802375697e-231, 4.65988169467104e-155, 0.17570681826216974, 3.6799428630948515e-78, 1.1457624253715345e-231, 0, 9.649381542291953e-232, 3.0201969628847042e-232, 1.072268715173723e-231, 4.326526591444836e-155, 4.849829366726974e-155, 5.186903763466333e-155, 5.952774829701863e-155, 2.3160821182153625e-232, 1.0772479586757503e-231, 1.1896457329133973e-231, 1.1200407237786664e-231, 0, 0.13534889927489716, 1.1988328686372911e-231, 9.918706012922318e-232, 3.990275362536988e-78, 1.3682868820983658e-231, 3.103937230575946e-155, 0.21972813874997157, 1.6723365610461308e-78, 1.2340561512781763e-231, 0.33124989794664067, 0.19692215902857166, 1.8379236026302172e-78, 6.061838450024688e-155, 7.677476567795425e-155, 0.3291598889023262, 0.06722075912445491, 0.2224246939793677, 1.0518351895246305e-231, 1.0832677820940877e-231, 1.9348698879863898e-78, 2.7793391731301436e-155, 1.2508498911928379e-231, 8.20974652257669e-232, 3.013810011652889e-78, 3.0099114964285766e-155, 0, 2.2979225395874126e-78, 6.13274920178966e-155, 4.729852449410561e-155, 4.079365729247308e-155, 0.4341999352730603, 5.4202718404926415e-155, 5.184172266295865e-155, 9.788429383461836e-232, 4.7842543848248265e-155, 0.15911783110981517, 6.725149025315138e-232, 1.2183324802375697e-231, 1.3657076305145602e-231, 2.9559907859786378e-78, 1.826959830028561e-233, 0.3438931217657843, 1.2035620841904511e-231, 1.0669733992029681e-231, 0.212226747319313, 5.554837769749797e-155, 9.109159947227211e-232, 5.019362538473707e-155, 6.994102472358586e-232, 3.6596954424873074e-155, 4.997414500212054e-155, 5.859491021246916e-155, 6.613992089093711e-232, 9.048991276198851e-233, 4.532281369102089e-155, 6.757037236273219e-155, 1.0244914152188952e-231, 2.0525740623205455e-78, 1.2183324802375697e-231, 1.2183324802375697e-231, 1.1358893931255172e-78, 5.74867560479262e-155, 2.8396123483926595e-78, 5.767166022187344e-155, 7.053663163619216e-155, 1.92015905099804e-78, 2.38410937715959e-78, 0, 1.3568676344828118e-231, 1.268852357850863e-231, 2.73371006502975e-78, 1.2183324802375697e-231, 0.11610218285945821, 4.532281369102089e-155, 1.1770467245389452e-231, 2.627904675175635e-78, 9.125821625692123e-232, 1.1475927822286276e-231, 1.2356563982938026e-231, 1.2039080317728252e-155, 1.2387197655613557e-231, 4.390825315614416e-155, 6.709616501577743e-155, 5.06160493724778e-155, 3.6170146665513074e-78, 0, 2.834320333835883e-155, 1.076601202255194e-231, 0, 4.084585940286461e-232, 6.7393716283177006e-155, 0.18185234853618296, 1.8144734173292848e-155, 1.8827690320009904e-155, 3.5894169220552947e-155, 1.6278257593311415e-78, 0.15048958015468575, 5.157006819435075e-155, 7.916504037467065e-232, 4.3233341924983234e-155, 1.0016022933125248e-231, 1.2679317774590278e-231, 5.359624986283916e-155]\n",
      "ROUGE Scores: {'rouge-1': {'r': 0.31022534927385753, 'p': 0.31451232202295365, 'f': 0.2984100924626969}, 'rouge-2': {'r': 0.08481806391462593, 'p': 0.08777826191950457, 'f': 0.0808576882745563}, 'rouge-l': {'r': 0.26898844182872333, 'p': 0.27267799813222426, 'f': 0.25839515931875}}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge import Rouge\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from tqdm import tqdm  # Progress bar for monitoring\n",
    "\n",
    "# Load the test dataset\n",
    "file_path = './train_data.csv'\n",
    "test_data = pd.read_csv(file_path)\n",
    "\n",
    "# Extract questions (HUMAN) and reference answers (ASSISTANT)\n",
    "questions = []\n",
    "reference_answers = []\n",
    "\n",
    "for text in test_data['text']:\n",
    "    try:\n",
    "        human_part = text.split('### Human:')[1].split('### Assistant:')[0].strip()\n",
    "        assistant_part = text.split('### Assistant:')[1].strip()\n",
    "        questions.append(human_part)\n",
    "        reference_answers.append(assistant_part)\n",
    "    except IndexError:\n",
    "        # Handle the case where the format is not as expected\n",
    "        print(f\"Skipping malformed text: {text}\")\n",
    "\n",
    "\n",
    "# Function to generate responses\n",
    "def generate_response(question: str) -> str:\n",
    "    prompt = f\"<human>: {question}\\n<assistant>:\"\n",
    "    encoding = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    with torch.inference_mode():\n",
    "        outputs = model.generate(\n",
    "            input_ids=encoding.input_ids,\n",
    "            attention_mask=encoding.attention_mask,\n",
    "            max_length=100,  # adjust as needed\n",
    "            num_return_sequences=1,\n",
    "            no_repeat_ngram_size=2,\n",
    "            early_stopping=True\n",
    "        )\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    assistant_start = \"<assistant>:\"\n",
    "    response_start = response.find(assistant_start)\n",
    "    return response[response_start + len(assistant_start):].strip() if response_start != -1 else response.strip()\n",
    "\n",
    "# Generate responses for all questions\n",
    "generated_answers = []\n",
    "for question in tqdm(questions, desc=\"Generating responses\"):\n",
    "    generated_answers.append(generate_response(question))\n",
    "\n",
    "# Calculate BLEU scores\n",
    "bleu_scores = [sentence_bleu([ref.split()], gen.split()) for ref, gen in zip(reference_answers, generated_answers)]\n",
    "\n",
    "# Calculate ROUGE scores\n",
    "rouge = Rouge()\n",
    "rouge_scores = rouge.get_scores(generated_answers, reference_answers, avg=True)\n",
    "\n",
    "# Print BLEU and ROUGE scores\n",
    "print(\"BLEU Scores:\", bleu_scores)\n",
    "print(\"ROUGE Scores:\", rouge_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\BMSCE CSE\\Desktop\\Instruct\\myenv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\BMSCE CSE\\Desktop\\Instruct\\myenv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is important to split the cluster definition file into additional files because it makes the code more organized and easier to maintain. It also allows for better scalability and flexibility in the future.\n",
      "User\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Why is it important to split the cluster definition file into additional files?\"\n",
    "print(generate_response(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The command used to generate a cluster definition file is \"kubectl create cluster --name <cluster-name> --zones <zones> --overrides <overrides>\".\n",
      "User\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prompt = \"Can you name the command used to generate a cluster definition file?\"\n",
    "print(generate_response(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, I don't have that information. Is there anything else I can help you with?\n",
      "User\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prompt = \"How many nodes were scanned from ILOCM 172.24.5.4 and 172.24.5.5 combined?\"\n",
    "print(generate_response(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can write the cluster definition file in any directory that you have write permissions.\n",
      "User\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Where can you write the cluster definition file after generating it?\"\n",
    "print(generate_response(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A cluster definition file contains information about the cluster, such as the cluster name, the number of nodes in the cluster, and the configuration settings for the cluster.\n",
      "User\n"
     ]
    }
   ],
   "source": [
    " \n",
    "\n",
    "prompt = \"What does a cluster definition file contain\"\n",
    "print(generate_response(prompt))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
